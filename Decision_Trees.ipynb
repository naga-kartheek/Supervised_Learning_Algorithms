{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38083cdd",
   "metadata": {},
   "source": [
    "### ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9735de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99847b1f",
   "metadata": {},
   "source": [
    "#### Entropy:\n",
    "#### H(S) = -$\\sum_{i=1}^{c}$ pi * $ \\log_2(pi)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f23e757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    unique_class, class_counts = np.unique(target_col, return_counts=True)\n",
    "    p = class_counts / len(target_col)\n",
    "    entr = -np.sum(p * np.log2(p))\n",
    "    return entr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c04e92",
   "metadata": {},
   "source": [
    "#### Information Gain = Entropy(Parent) - Average entropy(Children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "52668f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data, split_attribute_name, target_name=\"class\"):\n",
    "    # Calculate the total entropy before splitting\n",
    "    total_entr = entropy(data[target_name])\n",
    "\n",
    "    # Calculate the values and counts for the split attribute\n",
    "    v, c = np.unique(data[split_attribute_name], return_counts=True)\n",
    "\n",
    "    # Calculate the weighted average after splitting\n",
    "    weighted_avg = np.sum([(c[j] / np.sum(c)) * entropy(data[data[split_attribute_name]==val][target_name]) for j,val in enumerate(v)])\n",
    "    return total_entr - weighted_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0091a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, originaldata, features, target_attribute_name=\"class\", parent_node_class = None):\n",
    "    # If all target variable or y values have the same value, then\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "\n",
    "    # If the dataset is empty, then we can return the most frequently occuring target variable value in the original dataset.\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name], return_counts=True)[1])]\n",
    "\n",
    "    # If the feature space is empty, then return parent node class\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "\n",
    "    parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]\n",
    "\n",
    "    # Select the feature which best splits the dataset\n",
    "    it = [InfoGain(data, f, target_attribute_name) for f in features]  # Calculate the information gain for every feature\n",
    "    bf_index = np.argmax(it)\n",
    "    best_feature = features[bf_index]\n",
    "    tree = {best_feature: {}}\n",
    "    \n",
    "    features = [j for j in features if j != best_feature]\n",
    "    uni = np.unique(data[best_feature])\n",
    "\n",
    "    for k in uni:\n",
    "        # Split the dataset along the value of the feature variable with the largest information gain and create sub datasets.\n",
    "        sub_data = data[data[best_feature] == k]\n",
    "        subtree = ID3(sub_data, data, features, target_attribute_name, parent_node_class)\n",
    "        tree[best_feature][k] = subtree\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261f449",
   "metadata": {},
   "source": [
    "### Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "40308d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    print('Dimension of X: ', X.shape)\n",
    "    print('Dimension of y: ', y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "    \n",
    "    print('Dimension of X_train: ', X_train.shape)\n",
    "    print('Dimension of X_test: ', X_test.shape)\n",
    "    print('Dimension of y_train: ', y_train.shape)\n",
    "    print('Dimension of y_test: ', y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a23909",
   "metadata": {},
   "source": [
    "### Training and prediction of ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8a6f1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the ID3 model on the Iris dataset\n",
    "def train_id3_model(X_train, y_train, features):\n",
    "    d = pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=\"class\")], axis=1)\n",
    "    id3_model = ID3(d, d, features, target_attribute_name=\"class\", parent_node_class=None)\n",
    "    return id3_model\n",
    "\n",
    "\n",
    "# Predict the class of samples using the trained ID3 model\n",
    "def predict_id3(tree, instance):\n",
    "    \n",
    "    if isinstance(tree, dict):\n",
    "        root = list(tree.keys())[0]\n",
    "        val = instance[root]\n",
    "        if val not in tree[root]:\n",
    "            return np.argmax(np.bincount(y_train))\n",
    "        subtree = tree[root][val]\n",
    "        return predict_id3(subtree, instance)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0a568",
   "metadata": {},
   "source": [
    "### Evaluation metrics of ID3 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2e84c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X:  (150, 4)\n",
      "Dimension of y:  (150,)\n",
      "Dimension of X_train:  (120, 4)\n",
      "Dimension of X_test:  (30, 4)\n",
      "Dimension of y_train:  (120,)\n",
      "Dimension of y_test:  (30,)\n",
      "Dimension of y_test_pred:  (30,)\n",
      "-------------------------------------\n",
      "Confusion Matrix: \n",
      " [[11  0  0]\n",
      " [ 0  5  1]\n",
      " [ 0  3 10]]\n",
      "ID3 Test Accuracy: 86.67 %\n",
      "ID3 Test Precision: 84.47 %\n",
      "ID3 Test Recall: 86.75 %\n",
      "ID3 Test F1 Score: 84.92 %\n"
     ]
    }
   ],
   "source": [
    "def evaluate_id3_model(tree, X_test, y_test):\n",
    "\n",
    "    y_test_pred = np.array([predict_id3(tree, instance) for instance in X_test])\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    confusion_mat = confusion_matrix(y_test,y_test_pred)\n",
    "    \n",
    "    print('Dimension of y_test_pred: ', y_test_pred.shape)\n",
    "    print('-------------------------------------')\n",
    "    print(\"Confusion Matrix: \\n\", confusion_mat)\n",
    "    print(\"ID3 Test Accuracy:\", round(accuracy* 100, 2),\"%\")\n",
    "    print(\"ID3 Test Precision:\", round(precision* 100, 2),\"%\")\n",
    "    print(\"ID3 Test Recall:\", round(recall* 100, 2),\"%\")\n",
    "    print(\"ID3 Test F1 Score:\", round(f1* 100, 2),\"%\")\n",
    "    \n",
    "# Train and evaluate ID3 model\n",
    "X_train, X_test, y_train, y_test = load_and_prepare_data()\n",
    "id3_tree = train_id3_model(X_train, y_train, list(range(X_train.shape[1])))\n",
    "evaluate_id3_model(id3_tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1625e49e",
   "metadata": {},
   "source": [
    "## C4.5 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "592027db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    unique_class, class_counts = np.unique(target_col, return_counts=True)\n",
    "    p = class_counts / len(target_col)\n",
    "    entr = -np.sum(p * np.log2(p))\n",
    "    return entr  \n",
    "\n",
    "\n",
    "def info_gain(data, split_attribute_name, target_name=\"class\"):\n",
    "    # Calculate the total entropy before splitting\n",
    "    total_entr = entropy(data[target_name])\n",
    "\n",
    "    # Calculate the values and counts for the split attribute\n",
    "    v, c = np.unique(data[split_attribute_name], return_counts=True)\n",
    "\n",
    "    # Calculate the weighted average after splitting\n",
    "    weighted_avg = np.sum([(c[j] / np.sum(c)) * entropy(data[data[split_attribute_name]==val][target_name]) for j,val in enumerate(v)])\n",
    "    return total_entr - weighted_avg  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b9810",
   "metadata": {},
   "source": [
    "#### Gain ratio = Info gain/split info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9ff38f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio(data, split_attribute_name, target_name=\"class\"):\n",
    "    gain = info_gain(data, split_attribute_name, target_name)\n",
    "    split_info = entropy(data[split_attribute_name])\n",
    "    if split_info !=0 :\n",
    "        return gain / split_info\n",
    "    return 0 \n",
    "\n",
    "\n",
    "def best_split(data, features, target_name=\"class\"):\n",
    "    gain_rat = [gain_ratio(data, f, target_name) for f in features]\n",
    "    bf_ind = np.argmax(gain_rat)\n",
    "    return features[bf_ind] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c86ad276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def C45(data, features, target_name=\"class\", parent_node_class=None):\n",
    "    if len(np.unique(data[target_name])) <= 1:\n",
    "        return np.unique(data[target_name])[0]\n",
    "    elif len(data) == 0:\n",
    "        return np.unique(parent_node_class)[np.argmax(np.unique(parent_node_class, return_counts=True)[1])]\n",
    "    elif len(features) == 0:\n",
    "        return parent_node_class\n",
    "\n",
    "    # Select the best feature to split on based on the gain ratio\n",
    "    best_feature = best_split(data, features, target_name)\n",
    "    tree = {best_feature: {}}\n",
    "\n",
    "    # Remove the selected best feature from the feature space\n",
    "    features = [f for f in features if f != best_feature]\n",
    "\n",
    "    # Split the dataset based on the selected feature\n",
    "    unique_values = np.unique(data[best_feature])\n",
    "    for val in unique_values:\n",
    "        sub_data = data[data[best_feature] == val]\n",
    "        subtree = C45(sub_data, features, target_name, parent_node_class)\n",
    "        tree[best_feature][val] = subtree\n",
    "\n",
    "    return tree  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47b52a",
   "metadata": {},
   "source": [
    "### Loading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46fa9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    \n",
    "    print('Dimension of X: ', X.shape)\n",
    "    print('Dimension of y: ', y.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "    \n",
    "    print('Dimension of X_train: ', X_train.shape)\n",
    "    print('Dimension of X_test: ', X_test.shape)\n",
    "    print('Dimension of y_train: ', y_train.shape)\n",
    "    print('Dimension of y_test: ', y_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d3595",
   "metadata": {},
   "source": [
    "### Training and prediction of C4.5 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4b9e04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the C4.5 model on the Iris dataset\n",
    "def train_C45_model(X_train, y_train, features):\n",
    "    x = pd.concat([pd.DataFrame(X_train), pd.Series(y_train, name=\"class\")], axis=1)\n",
    "    c45_model = C45(x, features, target_name=\"class\")\n",
    "    return c45_model \n",
    "\n",
    "# Predict the class of samples using the trained C4.5 model\n",
    "def predict_C45(tree, instance):\n",
    "     if isinstance(tree, dict):\n",
    "        root = list(tree.keys())[0]\n",
    "        val = instance[root]\n",
    "        if val not in tree[root]:\n",
    "            return np.argmax(np.bincount(y_train))\n",
    "        subtree = tree[root][val]\n",
    "        return predict_C45(subtree, instance)\n",
    "     return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e9811",
   "metadata": {},
   "source": [
    "### Evaluation metrics of C4.5 Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "013e5d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of X:  (150, 4)\n",
      "Dimension of y:  (150,)\n",
      "Dimension of X_train:  (120, 4)\n",
      "Dimension of X_test:  (30, 4)\n",
      "Dimension of y_train:  (120,)\n",
      "Dimension of y_test:  (30,)\n",
      "Dimension of y_test_pred:  (30,)\n",
      "-------------------------------------\n",
      "Confusion Matrix: \n",
      " [[11  0  0]\n",
      " [ 0  5  1]\n",
      " [ 0  2 11]]\n",
      "C4.5 Test Accuracy: 90.0 %\n",
      "C4.5 Test Precision: 87.7 %\n",
      "C4.5 Test Recall: 89.32 %\n",
      "C4.5 Test F1 Score: 88.31 %\n"
     ]
    }
   ],
   "source": [
    "def evaluate_C45_model(tree, X_test, y_test):\n",
    "    y_test_pred = np.array([predict_C45(tree, instance) for instance in X_test])\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_test_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "    confusion_mat = confusion_matrix(y_test,y_test_pred)\n",
    "    \n",
    "    print('Dimension of y_test_pred: ', y_test_pred.shape)\n",
    "    print('-------------------------------------')\n",
    "    print(\"Confusion Matrix: \\n\", confusion_mat)\n",
    "    print(\"C4.5 Test Accuracy:\", round(accuracy* 100, 2),\"%\")\n",
    "    print(\"C4.5 Test Precision:\", round(precision* 100, 2),\"%\")\n",
    "    print(\"C4.5 Test Recall:\", round(recall* 100, 2),\"%\")\n",
    "    print(\"C4.5 Test F1 Score:\", round(f1* 100, 2),\"%\")\n",
    "    \n",
    "\n",
    "# Load and prepare the dataset\n",
    "X_train, X_test, y_train, y_test = load_and_prepare_data()\n",
    "\n",
    "# Define all four features\n",
    "features = list(range(X_train.shape[1]))\n",
    "\n",
    "# Train the C4.5 model\n",
    "c45_tree = train_C45_model(X_train, y_train, features)\n",
    "\n",
    "# Predict and evaluate the model \n",
    "evaluate_C45_model(c45_tree, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
